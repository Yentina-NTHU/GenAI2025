{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"載入原始數據\"\"\"\n",
    "    print(\"載入原始數據...\")\n",
    "    # 讀取數據\n",
    "    account_info = pd.read_csv('./Train/train_account_info.csv')\n",
    "    customer_info = pd.read_csv('./Train/train_customer_info.csv')\n",
    "    account_transactions = pd.read_csv('./Train/train_account_transactions.csv')\n",
    "    suspicious_accounts = pd.read_csv('./Train/train_suspicious_accounts.csv')\n",
    "    \n",
    "    print(f\"Account Info 形狀: {account_info.shape}\")\n",
    "    print(f\"Account Transactions 形狀: {account_transactions.shape}\")\n",
    "    print(f\"Customer Info 形狀: {customer_info.shape}\")\n",
    "    print(f\"Suspicious Accounts 形狀: {suspicious_accounts.shape}\")\n",
    "    \n",
    "    return account_info, customer_info, account_transactions, suspicious_accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_basic_data(account_info, customer_info):\n",
    "    \"\"\"合併基本帳戶和客戶信息\"\"\"\n",
    "    print(\"合併基本數據...\")\n",
    "    # 創建客戶-帳戶的對應關係\n",
    "    account_customer_map = account_info[['account_number', 'customer_id']].drop_duplicates()\n",
    "    print(f\"客戶-帳戶映射表形狀: {account_customer_map.shape}\")\n",
    "    \n",
    "    # 合併客戶資訊\n",
    "    merged_data = pd.merge(\n",
    "        account_customer_map, \n",
    "        customer_info, \n",
    "        on='customer_id', \n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"合併客戶資訊後的形狀: {merged_data.shape}\")\n",
    "    \n",
    "    # 合併帳戶資訊\n",
    "    merged_data = pd.merge(\n",
    "        merged_data,\n",
    "        account_info.drop('customer_id', axis=1),\n",
    "        on='account_number',\n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"合併帳戶資訊後的形狀: {merged_data.shape}\")\n",
    "    \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transaction_features(account_transactions):\n",
    "    \"\"\"從交易數據中提取特徵\"\"\"\n",
    "    print(\"從交易數據提取特徵...\")\n",
    "    \n",
    "    # 1. 基本交易統計特徵\n",
    "    transaction_count = account_transactions.groupby('account_number').size().reset_index(name='transaction_count')\n",
    "    transaction_amount_mean = account_transactions.groupby('account_number')['transaction_amount'].mean().reset_index(name='avg_transaction_amount')\n",
    "    transaction_amount_sum = account_transactions.groupby('account_number')['transaction_amount'].sum().reset_index(name='total_transaction_amount')\n",
    "    transaction_amount_std = account_transactions.groupby('account_number')['transaction_amount'].std().reset_index(name='std_transaction_amount')\n",
    "    transaction_amount_max = account_transactions.groupby('account_number')['transaction_amount'].max().reset_index(name='max_transaction_amount')\n",
    "    \n",
    "    # 2. 交易方向特徵\n",
    "    transaction_direction = account_transactions.groupby('account_number')['transaction_direction'].mean().reset_index(name='income_ratio')\n",
    "    \n",
    "    # 3. 銀行使用行為特徵\n",
    "    mobile_banking_usage = account_transactions.groupby('account_number')['mobile_banking_check_count'].sum().reset_index(name='mobile_banking_usage')\n",
    "    ebanking_usage = account_transactions.groupby('account_number')['ebanking_check_count'].sum().reset_index(name='ebanking_usage')\n",
    "    same_ip_ratio = account_transactions.groupby('account_number')['is_same_ip'].mean().reset_index(name='same_ip_ratio')\n",
    "    same_device_ratio = account_transactions.groupby('account_number')['is_same_device'].mean().reset_index(name='same_device_ratio')\n",
    "    \n",
    "    # 4. 特殊交易特徵\n",
    "    # 與特定帳號的交易\n",
    "    account_bank_txn = account_transactions[account_transactions['counterparty_account'] == 'ACCT31429']\n",
    "    bank_txn_count = account_bank_txn.groupby('account_number').size().reset_index(name='bank_txn_count')\n",
    "    bank_txn_avg = account_bank_txn.groupby('account_number')['transaction_amount'].mean().reset_index(name='avg_bank_txn_amount')\n",
    "    \n",
    "    # 與特定客戶ID的交易\n",
    "    account_virtual_txn = account_transactions[account_transactions['counterparty_customer_id'] == 'ID99999']\n",
    "    virtual_txn_count = account_virtual_txn.groupby('account_number').size().reset_index(name='virtual_txn_count')\n",
    "    virtual_txn_avg = account_virtual_txn.groupby('account_number')['transaction_amount'].mean().reset_index(name='avg_virtual_txn_amount')\n",
    "    \n",
    "    # 5. 交易對方多樣性\n",
    "    counterparty_diversity = account_transactions.groupby('account_number')['counterparty_account'].nunique().reset_index(name='counterparty_diversity')\n",
    "    \n",
    "    # 6. 交易渠道和代碼使用次數\n",
    "    channel_usage = pd.get_dummies(account_transactions['transaction_channel']).groupby(account_transactions['account_number']).sum()\n",
    "    channel_usage.columns = [f'channel_{col}_count' for col in channel_usage.columns]\n",
    "    \n",
    "    code_usage = pd.get_dummies(account_transactions['transaction_code']).groupby(account_transactions['account_number']).sum()\n",
    "    code_usage.columns = [f'code_{col}_count' for col in code_usage.columns]\n",
    "    \n",
    "    # 合併所有交易特徵\n",
    "    transaction_features = transaction_count\n",
    "    features_to_merge = [\n",
    "        transaction_amount_mean, transaction_amount_sum, transaction_amount_std,\n",
    "        transaction_amount_max, transaction_direction, mobile_banking_usage,\n",
    "        ebanking_usage, same_ip_ratio, same_device_ratio, bank_txn_count,\n",
    "        bank_txn_avg, virtual_txn_count, virtual_txn_avg, counterparty_diversity\n",
    "    ]\n",
    "    \n",
    "    for feature in features_to_merge:\n",
    "        transaction_features = pd.merge(\n",
    "            transaction_features,\n",
    "            feature,\n",
    "            on='account_number',\n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "    # 合併渠道和代碼使用特徵\n",
    "    transaction_features = pd.merge(\n",
    "        transaction_features,\n",
    "        channel_usage.reset_index(),\n",
    "        on='account_number',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    transaction_features = pd.merge(\n",
    "        transaction_features,\n",
    "        code_usage.reset_index(),\n",
    "        on='account_number',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"交易特徵的形狀: {transaction_features.shape}\")\n",
    "    return transaction_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_transaction_features(merged_data, transaction_features):\n",
    "    \"\"\"將交易特徵合併到主數據集\"\"\"\n",
    "    print(\"合併交易特徵到主數據集...\")\n",
    "    merged_data_with_txn = pd.merge(\n",
    "        merged_data,\n",
    "        transaction_features,\n",
    "        on='account_number',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 填充缺失值\n",
    "    numeric_txn_cols = transaction_features.columns.drop('account_number')\n",
    "    for col in numeric_txn_cols:\n",
    "        merged_data_with_txn[col] = merged_data_with_txn[col].fillna(0)\n",
    "    \n",
    "    print(f\"合併後的數據形狀: {merged_data_with_txn.shape}\")\n",
    "    return merged_data_with_txn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_suspicious_flag(merged_data_with_txn, suspicious_accounts):\n",
    "    \"\"\"添加可疑帳戶標記\"\"\"\n",
    "    print(\"添加可疑帳戶標記...\")\n",
    "    suspicious_accounts_list = suspicious_accounts['account_number'].tolist()\n",
    "    \n",
    "    # 添加is_suspicious標記\n",
    "    merged_data_with_txn['is_suspicious'] = merged_data_with_txn['account_number'].apply(\n",
    "        lambda x: 1 if x in suspicious_accounts_list else 0\n",
    "    )\n",
    "    \n",
    "    # 顯示可疑帳戶數量統計\n",
    "    suspicious_count = merged_data_with_txn['is_suspicious'].sum()\n",
    "    total_accounts = len(merged_data_with_txn)\n",
    "    print(f\"可疑帳戶數量: {suspicious_count} ({suspicious_count/total_accounts:.2%})\")\n",
    "    print(f\"非可疑帳戶數量: {total_accounts - suspicious_count} ({(total_accounts - suspicious_count)/total_accounts:.2%})\")\n",
    "    \n",
    "    return merged_data_with_txn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_features(df):\n",
    "    \"\"\"創建進階特徵\"\"\"\n",
    "    print(\"創建進階特徵...\")\n",
    "    \n",
    "    # 1. 時間相關特徵\n",
    "    if 'account_open_date' in df.columns:\n",
    "        # 帳戶存在天數 (假設最新日期為18250)\n",
    "        df['account_age'] = 18250 - df['account_open_date']\n",
    "        \n",
    "        # 帳戶平均每日交易頻率\n",
    "        if 'transaction_count' in df.columns:\n",
    "            df['daily_txn_rate'] = df['transaction_count'] / (df['account_age'] + 1)\n",
    "            # 處理極端值\n",
    "            q99 = df['daily_txn_rate'].quantile(0.99)\n",
    "            df['daily_txn_rate'] = df['daily_txn_rate'].clip(upper=q99)\n",
    "    \n",
    "    # 2. 交易頻率與金額比率特徵\n",
    "    if all(col in df.columns for col in ['transaction_count', 'account_age']):\n",
    "        df['transaction_frequency'] = df['transaction_count'] / (df['account_age'] / 365 + 0.001)\n",
    "        # 處理極端值\n",
    "        q99 = df['transaction_frequency'].quantile(0.99)\n",
    "        df['transaction_frequency'] = df['transaction_frequency'].clip(upper=q99)\n",
    "    \n",
    "    if all(col in df.columns for col in ['avg_transaction_amount', 'aum_amt']):\n",
    "        df['avg_txn_to_aum_ratio'] = df['avg_transaction_amount'] / (df['aum_amt'] + 1)\n",
    "        # 處理極端值\n",
    "        q99 = df['avg_txn_to_aum_ratio'].quantile(0.99)\n",
    "        df['avg_txn_to_aum_ratio'] = df['avg_txn_to_aum_ratio'].clip(upper=q99)\n",
    "    \n",
    "    if all(col in df.columns for col in ['max_transaction_amount', 'aum_amt']):\n",
    "        df['max_txn_to_aum_ratio'] = df['max_transaction_amount'] / (df['aum_amt'] + 1)\n",
    "        # 處理極端值\n",
    "        q99 = df['max_txn_to_aum_ratio'].quantile(0.99)\n",
    "        df['max_txn_to_aum_ratio'] = df['max_txn_to_aum_ratio'].clip(upper=q99)\n",
    "    \n",
    "    if all(col in df.columns for col in ['max_transaction_amount', 'avg_transaction_amount']):\n",
    "        # 最大交易與平均交易金額比率\n",
    "        df['max_to_avg_ratio'] = df['max_transaction_amount'] / (df['avg_transaction_amount'] + 1)\n",
    "        q99 = df['max_to_avg_ratio'].quantile(0.99)\n",
    "        df['max_to_avg_ratio'] = df['max_to_avg_ratio'].clip(upper=q99)\n",
    "    \n",
    "    # 3. 交易行為異常指標\n",
    "    if all(col in df.columns for col in ['std_transaction_amount', 'avg_transaction_amount']):\n",
    "        df['txn_volatility'] = df['std_transaction_amount'] / (df['avg_transaction_amount'] + 1)\n",
    "        # 處理極端值\n",
    "        q99 = df['txn_volatility'].quantile(0.99)\n",
    "        df['txn_volatility'] = df['txn_volatility'].clip(upper=q99)\n",
    "    \n",
    "    # 4. 入帳與出帳的比例\n",
    "    if 'income_ratio' in df.columns:\n",
    "        # 避免除以零\n",
    "        outgoing_ratio = 1 - df['income_ratio']\n",
    "        outgoing_ratio = outgoing_ratio.replace(0, 0.001)\n",
    "        df['inout_ratio'] = df['income_ratio'] / outgoing_ratio\n",
    "        # 處理極端值\n",
    "        q99 = df['inout_ratio'].quantile(0.99)\n",
    "        df['inout_ratio'] = df['inout_ratio'].clip(upper=q99)\n",
    "    \n",
    "    # 5. 銀行交易比例\n",
    "    if all(col in df.columns for col in ['bank_txn_count', 'transaction_count']):\n",
    "        df['bank_txn_ratio'] = df['bank_txn_count'] / (df['transaction_count'] + 0.001)\n",
    "    \n",
    "    # 6. 網路行動銀行使用比例\n",
    "    if all(col in df.columns for col in ['mobile_banking_usage', 'ebanking_usage', 'transaction_count']):\n",
    "        df['digital_banking_ratio'] = (df['mobile_banking_usage'] + df['ebanking_usage']) / (df['transaction_count'] + 0.001)\n",
    "    \n",
    "    # 7. IP和設備行為異常指標\n",
    "    if all(col in df.columns for col in ['same_ip_ratio', 'same_device_ratio']):\n",
    "        df['device_change_score'] = (1 - df['same_ip_ratio']) + (1 - df['same_device_ratio'])\n",
    "        \n",
    "        # IP變更頻率和設備變更頻率\n",
    "        if 'transaction_count' in df.columns:\n",
    "            df['ip_change_freq'] = (1 - df['same_ip_ratio']) * df['transaction_count']\n",
    "            df['device_change_freq'] = (1 - df['same_device_ratio']) * df['transaction_count']\n",
    "    \n",
    "    # 8. 交易對象多樣性相關特徵\n",
    "    if all(col in df.columns for col in ['counterparty_diversity', 'transaction_count']):\n",
    "        df['counterparty_txn_ratio'] = df['counterparty_diversity'] / (df['transaction_count'] + 0.001)\n",
    "    \n",
    "    # 9. 交易渠道多樣性特徵\n",
    "    channel_columns = [col for col in df.columns if col.startswith('channel_') and col.endswith('_count')]\n",
    "    if channel_columns:\n",
    "        # 計算使用的渠道數量\n",
    "        df['channel_diversity'] = df[channel_columns].apply(lambda row: np.sum(row > 0), axis=1)\n",
    "        # 計算最常用渠道的使用比例\n",
    "        df['main_channel_ratio'] = df[channel_columns].apply(lambda row: np.max(row) / (np.sum(row) + 0.001), axis=1)\n",
    "    \n",
    "    # 10. 交易代碼多樣性\n",
    "    code_columns = [col for col in df.columns if col.startswith('code_') and col.endswith('_count')]\n",
    "    if code_columns:\n",
    "        # 計算使用的交易代碼數量\n",
    "        df['code_diversity'] = df[code_columns].apply(lambda row: np.sum(row > 0), axis=1)\n",
    "        # 計算不尋常交易代碼的使用次數 (假設code_45至code_54為不尋常代碼)\n",
    "        unusual_code_columns = [f'code_{i}_count' for i in range(45, 55) if f'code_{i}_count' in df.columns]\n",
    "        if unusual_code_columns:\n",
    "            df['unusual_code_usage'] = df[unusual_code_columns].sum(axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_and_outliers(df):\n",
    "    \"\"\"處理缺失值和異常值\"\"\"\n",
    "    print(\"處理缺失值和異常值...\")\n",
    "    \n",
    "    # 1. 處理數值型特徵的缺失值\n",
    "    numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    print(f\"發現 {len(numeric_columns)} 個數值型特徵\")\n",
    "    \n",
    "    # 使用中位數填充基本特徵的缺失值\n",
    "    for col in ['age', 'income_level', 'region_code']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # 2. 檢查並處理無限值和NaN\n",
    "    for col in numeric_columns:\n",
    "        # 檢查並替換無限值\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # 檢查NaN值並填充\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            print(f\"列 {col} 有 {nan_count} 個NaN值\")\n",
    "            df[col] = df[col].fillna(df[col].median() if df[col].notnull().any() else 0)\n",
    "    \n",
    "    # 3. 使用SimpleImputer處理剩餘缺失值\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "    \n",
    "    # 4. 檢查是否還有NaN\n",
    "    if df.isna().sum().sum() > 0:\n",
    "        print(f\"警告: 仍有 {df.isna().sum().sum()} 個NaN值，進行填充\")\n",
    "        df = df.fillna(0)\n",
    "    else:\n",
    "        print(\"所有NaN值已處理完成\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_high_correlation_features(X):\n",
    "    \"\"\"移除高度相關特徵\"\"\"\n",
    "    print(\"檢查和移除高度相關特徵...\")\n",
    "    \n",
    "    # 計算特徵相關性\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    high_corr_features = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    \n",
    "    # 顯示將被移除的高度相關特徵\n",
    "    if high_corr_features:\n",
    "        print(f\"將移除 {len(high_corr_features)} 個高度相關特徵: {high_corr_features[:5]}{'...' if len(high_corr_features) > 5 else ''}\")\n",
    "        X = X.drop(high_corr_features, axis=1)\n",
    "    else:\n",
    "        print(\"未發現高度相關特徵\")\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(X_train, X_test):\n",
    "    \"\"\"標準化特徵\"\"\"\n",
    "    print(\"標準化特徵...\")\n",
    "    \n",
    "    # 使用健壯縮放器，更能處理異常值\n",
    "    numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "    robust_scaler = RobustScaler()  # 使用健壯縮放代替標準縮放\n",
    "    \n",
    "    # 應用健壯縮放\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_train_scaled[numeric_features] = robust_scaler.fit_transform(X_train[numeric_features])\n",
    "    \n",
    "    # 測試集也進行健壯縮放\n",
    "    X_test_scaled = X_test.copy()\n",
    "    X_test_scaled[numeric_features] = robust_scaler.transform(X_test[numeric_features])\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, robust_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "載入原始數據...\n",
      "Account Info 形狀: (24969, 5)\n",
      "Account Transactions 形狀: (206333, 18)\n",
      "Customer Info 形狀: (23655, 5)\n",
      "Suspicious Accounts 形狀: (400, 3)\n",
      "合併基本數據...\n",
      "客戶-帳戶映射表形狀: (24969, 2)\n",
      "合併客戶資訊後的形狀: (24969, 6)\n",
      "合併帳戶資訊後的形狀: (24969, 9)\n",
      "從交易數據提取特徵...\n",
      "交易特徵的形狀: (24969, 85)\n",
      "合併交易特徵到主數據集...\n",
      "合併後的數據形狀: (24969, 93)\n"
     ]
    }
   ],
   "source": [
    "account_info, customer_info, account_transactions, suspicious_accounts = load_data()\n",
    "merged_data = merge_basic_data(account_info, customer_info)\n",
    "transaction_features = create_transaction_features(account_transactions)\n",
    "merged_data_with_txn = merge_transaction_features(merged_data, transaction_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 載入交易資料 ---\n",
    "txn = pd.read_csv('Train\\\\train_account_transactions.csv')\n",
    "\n",
    "# --- 2. 找出每一帳號最新交易日，計算與之的差距 ---\n",
    "latest_date = txn.groupby('account_number')['transaction_date'].max()\n",
    "txn = txn.merge(latest_date.rename('latest_date'), on='account_number')\n",
    "txn['days_diff'] = txn['latest_date'] - txn['transaction_date']\n",
    "\n",
    "# --- 3. 定義「觀測視窗」統計函式 ---\n",
    "def window_features(df, window):\n",
    "    dfw = df[df['days_diff'] <= window]                       # 篩選近 7 / 30 日\n",
    "    agg = {\n",
    "        'transaction_amount': ['count', 'sum', 'mean', 'max'],\n",
    "        'transaction_direction': ['mean'],                   # 1=入金,2=出金 → 比值概念\n",
    "        'transaction_hour':  lambda x: (x.isin(\n",
    "                             list(range(0,7))+[22,23])).mean(),  # 夜間交易比例\n",
    "        'is_same_ip':       lambda x: (1 - x).mean(),         # IP 變動率\n",
    "        'is_same_device':   lambda x: (1 - x).mean()          # 裝置變動率\n",
    "    }\n",
    "    grouped = dfw.groupby('account_number').agg(agg)\n",
    "    # 攤平欄位 + 加上視窗後綴\n",
    "    grouped.columns = ['_'.join([c[0], c[1] if isinstance(c[1], str) else 'lambda', f'{window}d'])\n",
    "                       for c in grouped.columns]\n",
    "    return grouped\n",
    "\n",
    "feat7  = window_features(txn,  7)   # 近一週\n",
    "feat30 = window_features(txn, 30)   # 近 30 天\n",
    "\n",
    "# --- 4. 合併兩個視窗 ---\n",
    "time_feats = feat7.join(feat30, how='outer').reset_index()    # 保留 account_number\n",
    "\n",
    "# --- 5. 併回原始資料 ---\n",
    "merged = (\n",
    "    merged_data_with_txn            \n",
    "    .merge(time_feats, on='account_number', how='left')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_number</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>aum_amt</th>\n",
       "      <th>age</th>\n",
       "      <th>income_level</th>\n",
       "      <th>region_code</th>\n",
       "      <th>is_unreachable</th>\n",
       "      <th>is_digital_account</th>\n",
       "      <th>account_open_date</th>\n",
       "      <th>transaction_count</th>\n",
       "      <th>...</th>\n",
       "      <th>code_45_count</th>\n",
       "      <th>code_46_count</th>\n",
       "      <th>code_47_count</th>\n",
       "      <th>code_48_count</th>\n",
       "      <th>code_49_count</th>\n",
       "      <th>code_50_count</th>\n",
       "      <th>code_51_count</th>\n",
       "      <th>code_52_count</th>\n",
       "      <th>code_53_count</th>\n",
       "      <th>code_54_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACCT6068</td>\n",
       "      <td>ID5684</td>\n",
       "      <td>256930</td>\n",
       "      <td>61</td>\n",
       "      <td>25.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8400</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACCT11459</td>\n",
       "      <td>ID10838</td>\n",
       "      <td>65</td>\n",
       "      <td>57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9569</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACCT15832</td>\n",
       "      <td>ID15012</td>\n",
       "      <td>14438</td>\n",
       "      <td>56</td>\n",
       "      <td>126.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8380</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACCT15612</td>\n",
       "      <td>ID14797</td>\n",
       "      <td>43872</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10610</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACCT18659</td>\n",
       "      <td>ID17677</td>\n",
       "      <td>2578166</td>\n",
       "      <td>72</td>\n",
       "      <td>25.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9666</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24964</th>\n",
       "      <td>ACCT18619</td>\n",
       "      <td>ID17645</td>\n",
       "      <td>2843</td>\n",
       "      <td>15</td>\n",
       "      <td>25.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18159</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24965</th>\n",
       "      <td>ACCT20836</td>\n",
       "      <td>ID19721</td>\n",
       "      <td>142</td>\n",
       "      <td>20</td>\n",
       "      <td>25.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17638</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24966</th>\n",
       "      <td>ACCT24372</td>\n",
       "      <td>ID23132</td>\n",
       "      <td>474</td>\n",
       "      <td>20</td>\n",
       "      <td>25.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17565</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24967</th>\n",
       "      <td>ACCT501</td>\n",
       "      <td>ID463</td>\n",
       "      <td>175</td>\n",
       "      <td>20</td>\n",
       "      <td>25.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18142</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24968</th>\n",
       "      <td>ACCT16300</td>\n",
       "      <td>ID15460</td>\n",
       "      <td>24430</td>\n",
       "      <td>36</td>\n",
       "      <td>25.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16254</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24969 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      account_number customer_id  aum_amt  age  income_level  region_code  \\\n",
       "0           ACCT6068      ID5684   256930   61          25.0         12.0   \n",
       "1          ACCT11459     ID10838       65   57           NaN         12.0   \n",
       "2          ACCT15832     ID15012    14438   56         126.0         12.0   \n",
       "3          ACCT15612     ID14797    43872   46           NaN         12.0   \n",
       "4          ACCT18659     ID17677  2578166   72          25.0         12.0   \n",
       "...              ...         ...      ...  ...           ...          ...   \n",
       "24964      ACCT18619     ID17645     2843   15          25.0         12.0   \n",
       "24965      ACCT20836     ID19721      142   20          25.0         12.0   \n",
       "24966      ACCT24372     ID23132      474   20          25.0         12.0   \n",
       "24967        ACCT501       ID463      175   20          25.0         12.0   \n",
       "24968      ACCT16300     ID15460    24430   36          25.0         12.0   \n",
       "\n",
       "       is_unreachable  is_digital_account  account_open_date  \\\n",
       "0                   0                   0               8400   \n",
       "1                   0                   0               9569   \n",
       "2                   0                   0               8380   \n",
       "3                   0                   0              10610   \n",
       "4                   0                   0               9666   \n",
       "...               ...                 ...                ...   \n",
       "24964               0                   0              18159   \n",
       "24965               0                   0              17638   \n",
       "24966               0                   0              17565   \n",
       "24967               0                   1              18142   \n",
       "24968               0                   1              16254   \n",
       "\n",
       "       transaction_count  ...  code_45_count  code_46_count  code_47_count  \\\n",
       "0                      2  ...              0              0              0   \n",
       "1                     12  ...              0              0              3   \n",
       "2                      6  ...              0              0              0   \n",
       "3                      2  ...              0              0              0   \n",
       "4                      9  ...              0              0              0   \n",
       "...                  ...  ...            ...            ...            ...   \n",
       "24964                 13  ...              0              0              7   \n",
       "24965                 71  ...              0              0              0   \n",
       "24966                 41  ...              0              0             16   \n",
       "24967                  6  ...              0              0              4   \n",
       "24968                 72  ...              0              0              0   \n",
       "\n",
       "       code_48_count  code_49_count  code_50_count  code_51_count  \\\n",
       "0                  0              0              0              0   \n",
       "1                  0              0              0              0   \n",
       "2                  0              0              0              0   \n",
       "3                  0              0              0              0   \n",
       "4                  0              0              0              0   \n",
       "...              ...            ...            ...            ...   \n",
       "24964              0              0              0              0   \n",
       "24965              0              0              0              0   \n",
       "24966              0              0              0              0   \n",
       "24967              0              0              0              0   \n",
       "24968              0              0              0              0   \n",
       "\n",
       "       code_52_count  code_53_count  code_54_count  \n",
       "0                  0              0              0  \n",
       "1                  0              0              0  \n",
       "2                  0              0              0  \n",
       "3                  0              0              0  \n",
       "4                  0              0              0  \n",
       "...              ...            ...            ...  \n",
       "24964              0              0              0  \n",
       "24965              0              0              0  \n",
       "24966              0              0              0  \n",
       "24967              0              0              0  \n",
       "24968              0              0              0  \n",
       "\n",
       "[24969 rows x 93 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data_with_txn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "添加可疑帳戶標記...\n",
      "可疑帳戶數量: 400 (1.60%)\n",
      "非可疑帳戶數量: 24569 (98.40%)\n"
     ]
    }
   ],
   "source": [
    "final_data = add_suspicious_flag(merged, suspicious_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "創建進階特徵...\n",
      "處理缺失值和異常值...\n",
      "發現 126 個數值型特徵\n",
      "所有NaN值已處理完成\n"
     ]
    }
   ],
   "source": [
    "final_data = create_advanced_features(final_data)\n",
    "final_data = handle_missing_and_outliers(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_data.drop(['is_suspicious', 'account_number', 'customer_id'], axis=1)\n",
    "y = final_data['is_suspicious']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM + scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 類別特徵轉型 ===\n",
    "cat_cols = [c for c in X.columns if\n",
    "            any(kw in c for kw in ['channel', 'code', 'weekday', 'region'])]  # ⚑ 自行確認\n",
    "for col in cat_cols:\n",
    "    X[col] = X[col].astype('category')\n",
    "\n",
    "# === 切分訓練 / 驗證 / 測試 ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val,  y_train, y_val  = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['region_code', 'channel_1_count', 'channel_2_count', 'channel_3_count', 'channel_4_count', 'channel_5_count', 'channel_6_count', 'channel_7_count', 'channel_8_count', 'channel_9_count', 'channel_10_count', 'channel_11_count', 'channel_12_count', 'channel_13_count', 'channel_14_count', 'channel_15_count', 'channel_16_count', 'channel_17_count', 'channel_18_count', 'channel_19_count', 'code_1_count', 'code_2_count', 'code_3_count', 'code_4_count', 'code_5_count', 'code_6_count', 'code_7_count', 'code_8_count', 'code_10_count', 'code_11_count', 'code_12_count', 'code_13_count', 'code_14_count', 'code_15_count', 'code_17_count', 'code_18_count', 'code_19_count', 'code_20_count', 'code_22_count', 'code_24_count', 'code_25_count', 'code_26_count', 'code_27_count', 'code_28_count', 'code_29_count', 'code_30_count', 'code_31_count', 'code_32_count', 'code_33_count', 'code_34_count', 'code_35_count', 'code_36_count', 'code_37_count', 'code_38_count', 'code_39_count', 'code_40_count', 'code_41_count', 'code_42_count', 'code_43_count', 'code_44_count', 'code_45_count', 'code_46_count', 'code_47_count', 'code_48_count', 'code_49_count', 'code_50_count', 'code_51_count', 'code_52_count', 'code_53_count', 'code_54_count', 'channel_diversity', 'main_channel_ratio', 'code_diversity', 'unusual_code_usage']\n"
     ]
    }
   ],
   "source": [
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight = 61.4\n"
     ]
    }
   ],
   "source": [
    "neg, pos = np.bincount(y_train)\n",
    "scale_pos_weight = neg / pos          # ≈ 60 ~ 65 (依資料而異)\n",
    "print(f\"scale_pos_weight = {scale_pos_weight:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[124]\tvalid_0's binary_logloss: 0.0331788\tvalid_0's f1: 0.681481\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(colsample_bytree=0.8, learning_rate=0.03, min_data_in_leaf=20,\n",
       "               n_estimators=4000, num_leaves=256, objective=&#x27;binary&#x27;,\n",
       "               random_state=42, scale_pos_weight=61.421875, subsample=0.8,\n",
       "               verbosity=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(colsample_bytree=0.8, learning_rate=0.03, min_data_in_leaf=20,\n",
       "               n_estimators=4000, num_leaves=256, objective=&#x27;binary&#x27;,\n",
       "               random_state=42, scale_pos_weight=61.421875, subsample=0.8,\n",
       "               verbosity=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.8, learning_rate=0.03, min_data_in_leaf=20,\n",
       "               n_estimators=4000, num_leaves=256, objective='binary',\n",
       "               random_state=42, scale_pos_weight=61.421875, subsample=0.8,\n",
       "               verbosity=-1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "lgbm_params = dict(\n",
    "    objective='binary',\n",
    "    # --- 失衡處理 ---\n",
    "    scale_pos_weight = scale_pos_weight,\n",
    "    # --- 基本結構 ---\n",
    "    num_leaves        = 256,          # ⚑ 視資料量‑維度可下修\n",
    "    max_depth         = -1,\n",
    "    learning_rate     = 0.03,\n",
    "    n_estimators      = 4000,         # 搭配 early‑stop\n",
    "    min_data_in_leaf  = 20,\n",
    "    subsample         = 0.8,\n",
    "    colsample_bytree  = 0.8,\n",
    "    random_state      = 42,\n",
    "    verbosity         = -1\n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(**lgbm_params)\n",
    "\n",
    "# --- 早停用 F1 / PR‑AUC 作指標 ---\n",
    "def f1_eval_lgb(y_true, y_pred):\n",
    "    y_pred_bin = (y_pred > 0.5).astype(int)\n",
    "    return 'f1', f1_score(y_true, y_pred_bin), True         # True=越高越好\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=f1_eval_lgb,                                # ⚑ 或 'average_precision'\n",
    "    categorical_feature=cat_cols,\n",
    "    callbacks=[lgb.early_stopping(200)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold = 0.356 ,  F1@val = 0.717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_val_prob = model.predict_proba(X_val)[:,1]\n",
    "precision, recall, thresh = precision_recall_curve(y_val, y_val_prob)\n",
    "f1 = 2*precision*recall / (precision+recall+1e-12)\n",
    "best_thr = thresh[f1.argmax()]\n",
    "print(f\"Best threshold = {best_thr:.3f} ,  F1@val = {f1.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric  : 51\n",
      "Categorical: 74\n"
     ]
    }
   ],
   "source": [
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "print(f\"Numeric  : {len(num_cols)}\\nCategorical: {len(cat_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.combine        import SMOTETomek\n",
    "from imblearn.pipeline       import Pipeline   # imblearn 的 Pipeline\n",
    "\n",
    "# --- 2‑1 數值欄位做縮放 (RobustScaler 對極端值較友善)\n",
    "numeric_tf   = Pipeline(steps=[('scaler', RobustScaler())])\n",
    "\n",
    "# --- 2‑2 類別欄位保持 category，不需額外 encoding (LGBM 會吃)\n",
    "categorical_tf = 'passthrough'\n",
    "\n",
    "# --- 2‑3 合併前處理 ---\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_tf , num_cols),\n",
    "        ('cat', 'passthrough', cat_cols)\n",
    "    ])\n",
    "\n",
    "# --- 2‑4 資料增強器：直接計算 categorical 索引 ---\n",
    "cat_start = len(num_cols)\n",
    "cat_idx   = list(range(cat_start, cat_start + len(cat_cols)))  # << 唯一修正處\n",
    "\n",
    "sampler = SMOTENC(\n",
    "    categorical_features = cat_idx,\n",
    "    random_state = 42,\n",
    "    n_jobs = -1\n",
    ")\n",
    "# 若要用 SMOTETomek，也把 cat_idx 傳進裡層 SMOTENC\n",
    "# sampler = SMOTETomek(\n",
    "#     smote = SMOTENC(categorical_features=cat_idx, random_state=42, n_jobs=-1),\n",
    "#     random_state = 42, n_jobs=-1\n",
    "# )\n",
    "\n",
    "# --- 2‑5 LightGBM ---\n",
    "lgbm = lgb.LGBMClassifier(**lgbm_params)\n",
    "\n",
    "# --- 2‑6 Pipeline ---\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('augment'   , sampler),\n",
    "    ('model'     , lgbm)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Wrong type(str) or unknown name(region_code) in categorical_feature",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 3‑1 Fit\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel__eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel__eval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf1_eval_lgb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel__categorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcat_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel__callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 3‑2 取得驗證機率\u001b[39;00m\n\u001b[0;32m      9\u001b[0m y_val_prob \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict_proba(X_val)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\imblearn\\pipeline.py:333\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    332\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 333\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, yt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\lightgbm\\sklearn.py:1560\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1557\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1558\u001b[0m             valid_sets\u001b[38;5;241m.\u001b[39mappend((valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y)))\n\u001b[1;32m-> 1560\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1046\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1047\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m-> 1049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mnum_feature()\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\lightgbm\\engine.py:297\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    299\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\lightgbm\\basic.py:3656\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[0;32m   3649\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_network(\n\u001b[0;32m   3650\u001b[0m         machines\u001b[38;5;241m=\u001b[39mmachines,\n\u001b[0;32m   3651\u001b[0m         local_listen_port\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_listen_port\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   3652\u001b[0m         listen_time_out\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[0;32m   3653\u001b[0m         num_machines\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_machines\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   3654\u001b[0m     )\n\u001b[0;32m   3655\u001b[0m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[1;32m-> 3656\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3657\u001b[0m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\lightgbm\\basic.py:2590\u001b[0m, in \u001b[0;36mDataset.construct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2585\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\n\u001b[0;32m   2586\u001b[0m                 predictor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, used_indices\u001b[38;5;241m=\u001b[39mused_indices\n\u001b[0;32m   2587\u001b[0m             )\n\u001b[0;32m   2588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2589\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[1;32m-> 2590\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2596\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[0;32m   2604\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\lightgbm\\basic.py:2153\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, feature_name, categorical_feature, params, position)\u001b[0m\n\u001b[0;32m   2151\u001b[0m         categorical_indices\u001b[38;5;241m.\u001b[39madd(name)\n\u001b[0;32m   2152\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong type(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(name)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) or unknown name(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) in categorical_feature\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m categorical_indices:\n\u001b[0;32m   2155\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cat_alias \u001b[38;5;129;01min\u001b[39;00m _ConfigAliases\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_feature\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: Wrong type(str) or unknown name(region_code) in categorical_feature"
     ]
    }
   ],
   "source": [
    "# 3‑1 Fit\n",
    "# 先算出 LGBM 看得到的「分類特徵索引」\n",
    "cat_feature_idx = list(range(len(num_cols),\n",
    "                             len(num_cols) + len(cat_cols)))\n",
    "\n",
    "# 然後把 pipeline 裝好\n",
    "clf = Pipeline([\n",
    "    ('prep', preprocess),\n",
    "    ('aug' , sampler) ,          # 如果 sampler is None 這行可拿掉\n",
    "    ('model', lgb.LGBMClassifier(**lgbm_params))\n",
    "])\n",
    "\n",
    "# 這裡只改 1 行：用索引而不是欄名\n",
    "clf.fit(X_train, y_train,\n",
    "        model__eval_set              = [(X_val, y_val)],\n",
    "        model__eval_metric           = f1_eval_lgb,\n",
    "        model__categorical_feature   = cat_feature_idx,   # ← ★\n",
    "        model__callbacks             = [lgb.early_stopping(200)])\n",
    "\n",
    "# 3‑2 取得驗證機率\n",
    "y_val_prob = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# 3‑3 找最佳 threshold (同前)\n",
    "precision, recall, thr = precision_recall_curve(y_val, y_val_prob)\n",
    "f1  = 2*precision*recall/(precision+recall+1e-12)\n",
    "best_thr = thr[f1.argmax()]\n",
    "print(f\"Val F1 = {f1.max():.3f} @ thr={best_thr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from collections import Counter\n",
    "\n",
    "def augment_data(X, y, method='smote', random_state=42):\n",
    "    \"\"\"\n",
    "    進行資料增強以提高可疑帳戶的 F1 分數\n",
    "    \n",
    "    參數:\n",
    "        X: 特徵資料集\n",
    "        y: 目標變數 (is_suspicious)\n",
    "        method: 增強方法\n",
    "            - 'smote': 合成少數類過採樣技術\n",
    "            - 'borderline_smote': 邊界 SMOTE (關注決策邊界附近的範例)\n",
    "            - 'adasyn': 自適應合成採樣\n",
    "            - 'smote_tomek': SMOTE + Tomek Links (過採樣+選擇性欠採樣)\n",
    "            - 'smote_enn': SMOTE + ENN (過採樣+編輯最近鄰欠採樣)\n",
    "            - 'hybrid': 自訂混合策略\n",
    "        random_state: 隨機種子，用於重現結果\n",
    "    \n",
    "    返回:\n",
    "        X_resampled, y_resampled: 重新採樣後的資料集\n",
    "    \"\"\"\n",
    "    print(f\"使用 {method} 進行資料增強...\")\n",
    "    print(f\"原始類別分佈: {Counter(y)}\")\n",
    "    \n",
    "    # 將資料拆分為訓練集和測試集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    if method == 'smote':\n",
    "        # 標準 SMOTE\n",
    "        sampling_strategy = 0.5  # 使少數類達到多數類的 50%\n",
    "        sampler = SMOTE(sampling_strategy=sampling_strategy, random_state=random_state)\n",
    "        X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    elif method == 'borderline_smote':\n",
    "        # 邊界 SMOTE (更關注決策邊界附近的樣本)\n",
    "        sampler = BorderlineSMOTE(\n",
    "            sampling_strategy=0.5, \n",
    "            random_state=random_state,\n",
    "            k_neighbors=5,\n",
    "            m_neighbors=10\n",
    "        )\n",
    "        X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    elif method == 'adasyn':\n",
    "        # ADASYN (根據局部密度自適應合成樣本)\n",
    "        sampler = ADASYN(sampling_strategy=0.5, random_state=random_state)\n",
    "        X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    elif method == 'smote_tomek':\n",
    "        # SMOTE + Tomek Links (結合過採樣和選擇性欠採樣)\n",
    "        sampler = SMOTETomek(sampling_strategy=0.5, random_state=random_state)\n",
    "        X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    elif method == 'smote_enn':\n",
    "        # SMOTE + ENN (過採樣後使用 ENN 清理可能的噪聲點)\n",
    "        sampler = SMOTEENN(sampling_strategy=0.5, random_state=random_state)\n",
    "        X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    elif method == 'hybrid':\n",
    "        # 自訂混合策略: 先進行欠採樣，再進行 SMOTE\n",
    "        \n",
    "        # 第一步: 對多數類進行欠採樣到其大小的 200%\n",
    "        undersampler = RandomUnderSampler(\n",
    "            sampling_strategy=lambda y: {0: int(Counter(y)[1] * 2), 1: Counter(y)[1]},\n",
    "            random_state=random_state\n",
    "        )\n",
    "        X_intermediate, y_intermediate = undersampler.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # 第二步: 使用 BorderlineSMOTE 生成邊界上的合成樣本\n",
    "        oversampler = BorderlineSMOTE(\n",
    "            sampling_strategy=0.8,  # 使少數類達到多數類的 80%\n",
    "            random_state=random_state\n",
    "        )\n",
    "        X_train_resampled, y_train_resampled = oversampler.fit_resample(X_intermediate, y_intermediate)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"未知的方法: {method}\")\n",
    "    \n",
    "    # 合併重採樣後的訓練集和原始測試集\n",
    "    X_resampled = pd.concat([pd.DataFrame(X_train_resampled), X_test.reset_index(drop=True)])\n",
    "    y_resampled = pd.concat([pd.Series(y_train_resampled), y_test.reset_index(drop=True)])\n",
    "    \n",
    "    print(f\"重採樣後的類別分佈: {Counter(y_resampled)}\")\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "def apply_full_augmentation_pipeline(X_train, y_train, random_state=42):\n",
    "    \"\"\"\n",
    "    應用完整的資料增強管道，包括多種技術的組合\n",
    "    \n",
    "    參數:\n",
    "        X: 特徵資料集\n",
    "        y: 目標變數 (is_suspicious)\n",
    "        random_state: 隨機種子\n",
    "    \n",
    "    返回:\n",
    "        X_augmented, y_augmented: 增強後的資料集\n",
    "    \"\"\"\n",
    "    smote = SMOTE(sampling_strategy=0.25, random_state=random_state)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # 2. Add very subtle feature enhancement\n",
    "    suspicious_indices = y_balanced[y_balanced == 1].index\n",
    "    key_features = ['txn_volatility', 'max_txn_to_aum_ratio', 'device_change_score']\n",
    "    \n",
    "    X_enhanced = X_balanced.copy()\n",
    "    for feature in key_features:\n",
    "        if feature in X_enhanced.columns:\n",
    "            std = X_enhanced[feature].std() * 0.2  # Much smaller enhancement\n",
    "            X_enhanced.loc[suspicious_indices, feature] += np.random.normal(0, std, size=len(suspicious_indices))\n",
    "    \n",
    "    return X_enhanced, y_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果要比較不同增強方法的效果，可以使用以下代碼:\n",
    "def compare_augmentation_methods(X, y, model_class, random_state=42):\n",
    "    \"\"\"\n",
    "    比較不同資料增強方法的效果\n",
    "    \n",
    "    參數:\n",
    "        X: 原始特徵資料集\n",
    "        y: 原始目標變數\n",
    "        model_class: 要使用的模型類別 (例如 RandomForestClassifier)\n",
    "        random_state: 隨機種子\n",
    "    \n",
    "    返回:\n",
    "        results: 包含各方法 F1 分數的字典\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import f1_score, classification_report\n",
    "    \n",
    "    # 要比較的方法\n",
    "    methods = ['original', 'smote', 'borderline_smote', 'adasyn', 'smote_tomek', 'smote_enn', 'hybrid', 'full_pipeline']\n",
    "    results = {}\n",
    "    \n",
    "    # 將資料拆分為訓練集和測試集 (使用相同的測試集進行公平比較)\n",
    "    X_train_original, X_test, y_train_original, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # 對於原始資料\n",
    "    model = model_class(random_state=random_state)\n",
    "    #model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "    model.fit(X_train_original, y_train_original)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results['original'] = f1\n",
    "    print(f\"原始資料 F1 分數: {f1:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # 對於各種增強方法\n",
    "    for method in methods[1:-1]:  # 跳過 'original' 和 'full_pipeline'\n",
    "        # 應用增強方法\n",
    "        X_resampled, y_resampled = augment_data(X_train_original, y_train_original, method=method, random_state=random_state)\n",
    "        \n",
    "        # 訓練模型\n",
    "        model = model_class(random_state=random_state)\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # 評估模型\n",
    "        y_pred = model.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        results[method] = f1\n",
    "        print(f\"{method} F1 分數: {f1:.4f}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # 對於完整管道\n",
    "    X_full_pipeline, y_full_pipeline = apply_full_augmentation_pipeline(X_train_original, y_train_original, random_state=random_state)\n",
    "    model = model_class(random_state=random_state)\n",
    "    model.fit(X_full_pipeline, y_full_pipeline)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results['full_pipeline'] = f1\n",
    "    print(f\"完整管道 F1 分數: {f1:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 比較不同方法 (可以根據需要取消註釋並使用)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始資料 F1 分數: 0.6034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00      4914\n",
      "         1.0       0.97      0.44      0.60        80\n",
      "\n",
      "    accuracy                           0.99      4994\n",
      "   macro avg       0.98      0.72      0.80      4994\n",
      "weighted avg       0.99      0.99      0.99      4994\n",
      "\n",
      "使用 smote 進行資料增強...\n",
      "原始類別分佈: Counter({0.0: 19655, 1.0: 320})\n",
      "重採樣後的類別分佈: Counter({0.0: 19655, 1.0: 7926})\n",
      "smote F1 分數: 0.6769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00      4914\n",
      "         1.0       0.88      0.55      0.68        80\n",
      "\n",
      "    accuracy                           0.99      4994\n",
      "   macro avg       0.94      0.77      0.84      4994\n",
      "weighted avg       0.99      0.99      0.99      4994\n",
      "\n",
      "使用 borderline_smote 進行資料增強...\n",
      "原始類別分佈: Counter({0.0: 19655, 1.0: 320})\n",
      "重採樣後的類別分佈: Counter({0.0: 19655, 1.0: 7926})\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_augmentation_methods\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRandomForestClassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[66], line 44\u001b[0m, in \u001b[0;36mcompare_augmentation_methods\u001b[1;34m(X, y, model_class, random_state)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 訓練模型\u001b[39;00m\n\u001b[0;32m     43\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m---> 44\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_resampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 評估模型\u001b[39;00m\n\u001b[0;32m     47\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hsuan\\anaconda3\\envs\\CS565600\\lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = compare_augmentation_methods(X, y, RandomForestClassifier, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_suspicious_features(X, y, suspicious_threshold=0.7):\n",
    "    \"\"\"\n",
    "    針對可疑帳戶特徵進行增強\n",
    "    \n",
    "    此函數針對已識別為可疑的帳戶進行特徵增強，使其模式更加明顯\n",
    "    \n",
    "    參數:\n",
    "        X: 特徵資料集\n",
    "        y: 目標變數 (is_suspicious)\n",
    "        suspicious_threshold: 增強的強度 (0-1)\n",
    "    \n",
    "    返回:\n",
    "        X_augmented: 增強後的特徵\n",
    "    \"\"\"\n",
    "    print(\"進行可疑特徵增強...\")\n",
    "    X_augmented = X.copy()\n",
    "    \n",
    "    # 取得可疑帳戶的索引\n",
    "    suspicious_indices = y[y == 1].index\n",
    "    \n",
    "    # 找出最具分辨力的特徵 (可基於領域知識增加或修改)\n",
    "    key_features = [\n",
    "        'txn_volatility', 'max_txn_to_aum_ratio', 'max_to_avg_ratio',\n",
    "        'inout_ratio', 'device_change_score', 'ip_change_freq',\n",
    "        'counterparty_txn_ratio', 'unusual_code_usage'\n",
    "    ]\n",
    "    \n",
    "    # 確保這些特徵存在於資料集中\n",
    "    valid_features = [f for f in key_features if f in X.columns]\n",
    "    \n",
    "    if not valid_features:\n",
    "        print(\"警告: 找不到關鍵的可疑特徵，跳過特徵增強\")\n",
    "        return X_augmented\n",
    "    \n",
    "    # 計算每個特徵的中位數和 IQR\n",
    "    medians = X[valid_features].median()\n",
    "    q75 = X[valid_features].quantile(0.75)\n",
    "    q25 = X[valid_features].quantile(0.25)\n",
    "    iqrs = q75 - q25\n",
    "    \n",
    "    # 對可疑帳戶的特徵進行增強\n",
    "    for feature in valid_features:\n",
    "        # 針對不同類型的特徵選擇增強方向\n",
    "        if feature in ['txn_volatility', 'max_txn_to_aum_ratio', 'max_to_avg_ratio',\n",
    "                       'device_change_score', 'ip_change_freq', 'unusual_code_usage']:\n",
    "            # 這些特徵越高越可疑，向上增強\n",
    "            enhancement = iqrs[feature] * suspicious_threshold\n",
    "            X_augmented.loc[suspicious_indices, feature] += np.random.uniform(0, enhancement, size=len(suspicious_indices))\n",
    "        \n",
    "        elif feature in ['same_ip_ratio', 'same_device_ratio']:\n",
    "            # 這些特徵越低越可疑，向下增強\n",
    "            enhancement = iqrs[feature] * suspicious_threshold\n",
    "            X_augmented.loc[suspicious_indices, feature] -= np.random.uniform(0, enhancement, size=len(suspicious_indices))\n",
    "            # 確保不低於0\n",
    "            X_augmented.loc[suspicious_indices, feature] = X_augmented.loc[suspicious_indices, feature].clip(lower=0)\n",
    "    \n",
    "    print(f\"已增強 {len(suspicious_indices)} 個可疑帳戶的特徵\")\n",
    "    return X_augmented\n",
    "\n",
    "def add_noise_to_majority(X, y, noise_level=0.05):\n",
    "    \"\"\"\n",
    "    給多數類添加微小噪聲，幫助模型更好地識別決策邊界\n",
    "    \n",
    "    參數:\n",
    "        X: 特徵資料集\n",
    "        y: 目標變數 (is_suspicious)\n",
    "        noise_level: 噪聲等級 (0-1)\n",
    "    \n",
    "    返回:\n",
    "        X_with_noise: 添加噪聲後的特徵\n",
    "    \"\"\"\n",
    "    print(\"對多數類添加微小噪聲...\")\n",
    "    X_with_noise = X.copy()\n",
    "    \n",
    "    # 取得非可疑帳戶的索引\n",
    "    normal_indices = y[y == 0].index\n",
    "    \n",
    "    # 為所有數值型特徵添加噪聲\n",
    "    numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        # 計算特徵的標準差\n",
    "        std = X[col].std()\n",
    "        \n",
    "        # 根據特徵標準差添加高斯噪聲\n",
    "        noise = np.random.normal(0, std * noise_level, size=len(normal_indices))\n",
    "        X_with_noise.loc[normal_indices, col] += noise\n",
    "    \n",
    "    print(f\"已對 {len(normal_indices)} 個非可疑帳戶添加微小噪聲\")\n",
    "    return X_with_noise\n",
    "\n",
    "def generate_synthetic_suspicious(X, y, n_samples=100, random_state=42):\n",
    "    \"\"\"\n",
    "    生成合成的可疑帳戶樣本，基於已知可疑帳戶的特徵分佈\n",
    "    \n",
    "    參數:\n",
    "        X: 特徵資料集\n",
    "        y: 目標變數 (is_suspicious)\n",
    "        n_samples: 要生成的合成樣本數量\n",
    "        random_state: 隨機種子\n",
    "    \n",
    "    返回:\n",
    "        X_synthetic, y_synthetic: 合成的樣本及其標籤\n",
    "    \"\"\"\n",
    "    print(f\"生成 {n_samples} 個合成可疑帳戶...\")\n",
    "    \n",
    "    # 提取所有可疑帳戶\n",
    "    suspicious_samples = X[y == 1]\n",
    "    \n",
    "    # 如果可疑樣本太少，則使用 SMOTE 生成更多樣本\n",
    "    if len(suspicious_samples) < 5:\n",
    "        print(\"可疑樣本數量太少，無法直接生成合成樣本，使用 SMOTE 生成中間樣本\")\n",
    "        smote = SMOTE(random_state=random_state)\n",
    "        X_intermediate, y_intermediate = smote.fit_resample(X, y)\n",
    "        suspicious_samples = X_intermediate[y_intermediate == 1]\n",
    "    \n",
    "    # 計算每個特徵的均值和協方差\n",
    "    numeric_cols = suspicious_samples.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    # 僅使用數值型特徵\n",
    "    suspicious_numeric = suspicious_samples[numeric_cols]\n",
    "    \n",
    "    # 計算均值和協方差\n",
    "    mean_vector = suspicious_numeric.mean().values\n",
    "    cov_matrix = suspicious_numeric.cov().values\n",
    "    \n",
    "    # 使用多變量高斯分佈生成合成樣本\n",
    "    np.random.seed(random_state)\n",
    "    synthetic_data = np.random.multivariate_normal(mean_vector, cov_matrix, n_samples)\n",
    "    \n",
    "    # 轉換為 DataFrame 並添加列名\n",
    "    X_synthetic = pd.DataFrame(synthetic_data, columns=numeric_cols)\n",
    "    \n",
    "    # 對於任何可能的非數值型特徵，我們從原始可疑樣本中隨機取樣\n",
    "    non_numeric_cols = [col for col in X.columns if col not in numeric_cols]\n",
    "    \n",
    "    if non_numeric_cols:\n",
    "        for col in non_numeric_cols:\n",
    "            random_values = suspicious_samples[col].sample(n=n_samples, replace=True, random_state=random_state).values\n",
    "            X_synthetic[col] = random_values\n",
    "    \n",
    "    # 所有合成樣本都標記為可疑 (1)\n",
    "    y_synthetic = pd.Series([1] * n_samples)\n",
    "    \n",
    "    return X_synthetic, y_synthetic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS565600",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
